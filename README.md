
# Web Scraping of Stock Market DataSet Using R : Documentation

## Overview
This project provides a comprehensive end-to-end data pipeline for stock market analysis. It covers various aspects, including data collection through web scraping, data wrangling, enrichment, and analysis. The ultimate objective is to generate a structured and enriched dataset that aids investment decision-making by leveraging visualizations and key performance metrics.

## Data Sources
The dataset is compiled from multiple reputable sources to ensure accuracy and comprehensiveness:

- **Wikipedia**:
  - **S&P 500 Companies**: Information including Symbol, GICS Sector, Sub-Industry, Headquarters, Date Added, CIK, and Founding Year.
  - **Dow Jones Industrial Average (DJIA) Companies**: Data on Company Name, Exchange, Symbol, Industry, Date Added, Notes, and Index Weighting.
- **Yahoo Finance API**:
  - Historical stock market data comprising daily opening price, high, low, closing price, and trading volume from 2019 to 2024.

A consolidated CSV file containing company names and their corresponding stock symbols has been generated by merging the above sources.

## Libraries Used
To perform data wrangling and analysis efficiently, the following R libraries were employed:

```r
library(dplyr)      # Data manipulation
library(stringr)    # String processing
library(lubridate)  # Date-time manipulation
library(ggplot2)    # Data visualization
library(rvest)      # Web scraping
library(httr)       # API requests
library(jsonlite)   # Handling JSON data
library(tidyr)      # Data cleaning and transformation
```

## Tools Used
To facilitate efficient data processing and analysis, the following tools were used:
- **R**: Primary programming language for data processing.
- **RStudio**: Integrated development environment for running and visualizing R scripts.
- **rvest**: Web scraping tool for extracting information from Wikipedia.
- **ggplot2**: Data visualization package used for exploratory analysis.

## Data Collection
The data collection phase consists of:
1. **Web Scraping**:
   - The functions `scrape_sp500` and `scrape_djia` were created to fetch and clean data from Wikipedia.
   - Extracted datasets were saved in CSV format (`sp500_data.csv`, `djia_data.csv`).
2. **Stock Data Retrieval**:
   - The `quantmod` package was used to fetch historical stock data from Yahoo Finance.
   - Each company's stock data was aggregated into yearly averages and stored as `yearly_averages.csv`.

## Data Wrangling & Enrichment
The dataset underwent rigorous cleaning and transformation to ensure accuracy and usability. Below are the key steps performed:

### 1. Discover
- Initial exploration of raw data sources to understand their structure and completeness.
- Identified missing values, inconsistencies, and data distribution patterns.

### 2. Structure
- Organized data into a structured format with appropriate column names and data types.
- Standardized key identifiers, such as stock symbols, to ensure consistency across sources.

### 3. Clean
- Removed duplicate entries and irrelevant columns.
- Standardized date formats for consistency.
- Extracted numerical values from financial columns.
- Addressed inconsistencies in stock price data.

### 4. Enrich
- Added derived features for better stock evaluation:
  - **Price Change Percentage**: `((avg_close - avg_open) / avg_open) * 100`
  - **Volatility Percentage**: `((avg_high - avg_low) / avg_open) * 100`
  - **Price Sentiment**: A categorical variable based on price movement and volatility.
  - **Investibility Score**: A custom metric balancing stock performance and risk.

### 5. Validate
- Conducted thorough checks for anomalies and inconsistencies in data values.
- Used statistical methods (IQR) to detect outliers and ensure data integrity.
- Ensured data completeness and consistency before proceeding to analysis.

### 6. Publish
- Final cleaned dataset was saved in a structured format (`final_merged_dataset.csv`).
- Generated summary reports and visualizations to highlight key insights.
- Prepared the dataset for downstream analysis and financial modeling.

## Data Characteristics
- **Time Span**: 2019-2024
- **Number of Records**: 500+ stocks
- **Key Metrics**:
  - Price Change Percentage
  - Volatility Percentage
  - Investibility Score

## Sales & Trends Analysis
The dataset enables in-depth stock market analysis through visualizations.

### Stock Price Distribution
![Stock Price Distribution](https://github.com/user-attachments/assets/4f535ff5-c3c4-4b3c-b2b4-7c8707b5eaaa)

### Sector-wise Performance
![Sector-wise Performance](https://github.com/user-attachments/assets/b7bb8d12-e4a2-47fc-893f-d90fb9001278)


### Most Volatile Stocks
![Most Volatile Stocks](https://github.com/user-attachments/assets/becc5c2b-e22f-4f65-b2e3-f9b6dc23a106)




## Potential Use Cases
The cleaned dataset can be utilized in various financial analyses and predictive modeling applications:

1. **Stock Trend Analysis**: Evaluate historical stock price trends and trading volumes.
2. **Investment Decision Making**: Identify profitable investment opportunities using the `investibility_score` metric.
3. **Risk Management**: Assess company volatility to make informed portfolio decisions.
4. **Financial Forecasting**: Develop predictive models based on stock performance data.

## Challenges & Resolutions
Several challenges were encountered during data collection and processing. Below are the key issues and solutions:

1. **Data Inconsistencies**:
   - Wikipedia tables had formatting variations across different pages.
   - Solution: Standardized column names before merging data.

2. **Handling Missing Data**:
   - Missing stock prices and financial metrics were common.
   - Solution: Median imputation for numerical values and assigning "Unknown" for categorical data.

3. **Outlier Handling**:
   - Extreme stock price values distorted analysis.
   - Solution: Outliers were detected using statistical methods (IQR) and adjusted where necessary.

4. **Merging Multiple Datasets**:
   - Different sources had inconsistencies in column names and formats.
   - Solution: Used a standardized key (`Symbol`) to ensure seamless merging.

## Final Deliverables
The project outputs include:
- **Final Cleaned Dataset (CSV format)**:
  - [`final_merged_dataset.csv`](https://github.com/yashnayan8795/Web-Scrapping-Stock-Dataset/blob/main/Dataset/final_merged_dataset.csv)
- **Summary Visualizations and Insights**:
  - Comprehensive data quality assessments and stock market trends.
    ![Price Change vs Volatility](https://github.com/user-attachments/assets/c4ff2227-4d4e-446d-bdd1-a624fa8b20d2)

---
This dataset provides valuable insights into market trends, investment strategies, and financial decision-making. By leveraging this dataset, analysts and investors can make data-driven decisions for effective stock portfolio management.

---
YASH NAYAN    22BDS0274
